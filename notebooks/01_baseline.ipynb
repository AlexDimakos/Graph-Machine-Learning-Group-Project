{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f850911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "path = Path(\"./data\")\n",
    "\n",
    "# Nodes\n",
    "nodes = pd.read_csv(path / Path(\"Nodes/Nodes.csv\"))\n",
    "edges = pd.read_csv(path / Path(\"Edges/Edges (Product Sub-Group).csv\"))\n",
    "\n",
    "delivery_to_distributor = pd.read_csv(\n",
    "    path / Path(\"Temporal Data/Unit/Delivery To distributor.csv\")\n",
    ")\n",
    "factory_issue = pd.read_csv(path / Path(\"Temporal Data/Unit/factory issue.csv\"))\n",
    "production = pd.read_csv(path / Path(\"Temporal Data/Unit/Production .csv\"))\n",
    "sales_order = pd.read_csv(path / Path(\"Temporal Data/Unit/Sales order.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2cc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (220, 41, 4)\n",
      "y shape: (220, 41)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric_temporal.signal import StaticGraphTemporalSignal\n",
    "\n",
    "signals_path = path / Path(\"Temporal Data/Unit/\")\n",
    "\n",
    "production = pd.read_csv(signals_path / Path(\"Production .csv\"))\n",
    "factory_issue = pd.read_csv(signals_path / Path(\"factory issue.csv\"))\n",
    "delivery = pd.read_csv(signals_path / Path(\"Delivery To distributor.csv\"))\n",
    "sales_order = pd.read_csv(signals_path / Path(\"Sales order.csv\"))\n",
    "\n",
    "products = [col for col in production.columns if col != \"Date\"]\n",
    "product_to_id = {prod: i for i, prod in enumerate(products)}\n",
    "\n",
    "\n",
    "def transform_temporal(df, mapping):\n",
    "    df_no_date = df.drop(columns=[\"Date\"])\n",
    "    return df_no_date.rename(columns=mapping)\n",
    "\n",
    "\n",
    "X_prod = transform_temporal(production, product_to_id)\n",
    "X_issue = transform_temporal(factory_issue, product_to_id)\n",
    "X_delivery = transform_temporal(delivery, product_to_id)\n",
    "X_sales = transform_temporal(sales_order, product_to_id)\n",
    "\n",
    "X_prod_np = X_prod.to_numpy()\n",
    "X_issue_np = X_issue.to_numpy()\n",
    "X_delivery_np = X_delivery.to_numpy()\n",
    "X_sales_np = X_sales.to_numpy()\n",
    "\n",
    "# shape: [T, N, F] = [time_steps, num_nodes, num_features]\n",
    "X = np.stack([X_prod_np, X_issue_np, X_delivery_np, X_sales_np], axis=-1)\n",
    "\n",
    "y = X_sales_np[1:]\n",
    "X = X[:-1]\n",
    "\n",
    "print(\"X shape:\", X.shape)  # (T-1, N, 4)\n",
    "print(\"y shape:\", y.shape)  # (T-1, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class GraphTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y, window_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: np.ndarray or torch.Tensor of shape [T, N, F]\n",
    "            y: np.ndarray or torch.Tensor of shape [T, N]\n",
    "            window_size: int, number of timesteps per input window\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(X):\n",
    "            X = torch.tensor(X, dtype=torch.float32)\n",
    "        if not torch.is_tensor(y):\n",
    "            y = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.window_size = window_size\n",
    "        self.T, self.N, self.F = X.shape\n",
    "\n",
    "        # total number of possible windows per node\n",
    "        self.num_windows_per_node = self.T - self.window_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # Each node has (T - window_size) samples\n",
    "        return self.num_windows_per_node * self.N\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Given a flat index, map to (node_id, time_index)\n",
    "        \"\"\"\n",
    "        node_id = idx // self.num_windows_per_node\n",
    "        t_start = idx % self.num_windows_per_node\n",
    "        t_end = t_start + self.window_size\n",
    "\n",
    "        x = self.X[t_start:t_end, node_id, :]  # [window_size, F]\n",
    "        y = self.y[t_end - 1, node_id]  # scalar target (last timestep)\n",
    "        return {\"x\": x, \"y\": y, \"node_id\": node_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf6a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class LSTMBaseline(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)  # predict scalar target\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [B, window_size, F]\n",
    "        out, _ = self.lstm(x)  # out: [B, window_size, hidden_size]\n",
    "        out = out[:, -1, :]  # take output of last timestep: [B, hidden_size]\n",
    "        out = self.fc(out)  # [B, 1]\n",
    "        return out.squeeze(-1)  # [B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05715e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataset, batch_size=64, criterion=None, device=None):\n",
    "    model.eval()\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    if criterion is None:\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_trues = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            y_true = batch[\"y\"].to(device)\n",
    "            y_pred = model(x)\n",
    "\n",
    "            loss = criterion(y_pred, y_true)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "            all_preds.append(y_pred.cpu())\n",
    "            all_trues.append(y_true.cpu())\n",
    "\n",
    "    avg_loss = total_loss / len(dataset)\n",
    "    all_preds = torch.cat(all_preds, dim=0)\n",
    "    all_trues = torch.cat(all_trues, dim=0)\n",
    "    rmse = torch.sqrt(torch.mean((all_preds - all_trues) ** 2))\n",
    "    return avg_loss, rmse.item(), all_preds, all_trues\n",
    "\n",
    "\n",
    "def train_lstm(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset=None,\n",
    "    window_size=12,\n",
    "    batch_size=64,\n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    eval_every=5,\n",
    "    patience=10,\n",
    "    save_path=\"best_model.pt\",\n",
    "):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_val_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Stats for plotting\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_rmse\": [],\n",
    "        \"best_val_loss\": np.inf,\n",
    "        \"best_val_rmse\": np.inf,\n",
    "    }\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            x = batch[\"x\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_dataset)\n",
    "        history[\"train_loss\"].append(avg_train_loss)\n",
    "\n",
    "        # Periodic evaluation\n",
    "        if val_dataset is not None and epoch % eval_every == 0:\n",
    "            val_loss, val_rmse, _, _ = evaluate_model(\n",
    "                model, val_dataset, batch_size, criterion, device\n",
    "            )\n",
    "            history[\"val_loss\"].append(val_loss)\n",
    "            history[\"val_rmse\"].append(val_rmse)\n",
    "            print(\n",
    "                f\"Epoch {epoch}/{num_epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {val_loss:.4f} | Val RMSE: {val_rmse:.4f}\"\n",
    "            )\n",
    "\n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                history[\"best_val_loss\"] = best_val_loss\n",
    "                history[\"best_val_rmse\"] = val_rmse\n",
    "                print(f\" --> Best model saved at epoch {epoch}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered.\")\n",
    "                    break\n",
    "        else:\n",
    "            print(f\"Epoch {epoch}/{num_epochs} | Train Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "    # Load best model\n",
    "    if val_dataset is not None:\n",
    "        model.load_state_dict(torch.load(save_path))\n",
    "\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7a143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 0.9565 | Val Loss: 3.2895 | Val RMSE: 1.8137\n",
      " --> Best model saved at epoch 1\n",
      "Epoch 2/100 | Train Loss: 0.9324 | Val Loss: 3.2841 | Val RMSE: 1.8122\n",
      " --> Best model saved at epoch 2\n",
      "Epoch 3/100 | Train Loss: 0.9213 | Val Loss: 3.2843 | Val RMSE: 1.8123\n",
      "Epoch 4/100 | Train Loss: 0.9160 | Val Loss: 3.2838 | Val RMSE: 1.8121\n",
      " --> Best model saved at epoch 4\n",
      "Epoch 5/100 | Train Loss: 0.9117 | Val Loss: 3.2859 | Val RMSE: 1.8127\n",
      "Epoch 6/100 | Train Loss: 0.9068 | Val Loss: 3.2817 | Val RMSE: 1.8115\n",
      " --> Best model saved at epoch 6\n",
      "Epoch 7/100 | Train Loss: 0.9006 | Val Loss: 3.2766 | Val RMSE: 1.8101\n",
      " --> Best model saved at epoch 7\n",
      "Epoch 8/100 | Train Loss: 0.8950 | Val Loss: 3.2765 | Val RMSE: 1.8101\n",
      " --> Best model saved at epoch 8\n",
      "Epoch 9/100 | Train Loss: 0.8877 | Val Loss: 3.2686 | Val RMSE: 1.8079\n",
      " --> Best model saved at epoch 9\n",
      "Epoch 10/100 | Train Loss: 0.8793 | Val Loss: 3.2614 | Val RMSE: 1.8059\n",
      " --> Best model saved at epoch 10\n",
      "Epoch 11/100 | Train Loss: 0.8691 | Val Loss: 3.2549 | Val RMSE: 1.8041\n",
      " --> Best model saved at epoch 11\n",
      "Epoch 12/100 | Train Loss: 0.8576 | Val Loss: 3.2542 | Val RMSE: 1.8039\n",
      " --> Best model saved at epoch 12\n",
      "Epoch 13/100 | Train Loss: 0.8424 | Val Loss: 3.2516 | Val RMSE: 1.8032\n",
      " --> Best model saved at epoch 13\n",
      "Epoch 14/100 | Train Loss: 0.8321 | Val Loss: 3.2549 | Val RMSE: 1.8041\n",
      "Epoch 15/100 | Train Loss: 0.8232 | Val Loss: 3.2588 | Val RMSE: 1.8052\n",
      "Epoch 16/100 | Train Loss: 0.8161 | Val Loss: 3.2551 | Val RMSE: 1.8042\n",
      "Epoch 17/100 | Train Loss: 0.8098 | Val Loss: 3.2497 | Val RMSE: 1.8027\n",
      " --> Best model saved at epoch 17\n",
      "Epoch 18/100 | Train Loss: 0.8043 | Val Loss: 3.2541 | Val RMSE: 1.8039\n",
      "Epoch 19/100 | Train Loss: 0.7980 | Val Loss: 3.2480 | Val RMSE: 1.8022\n",
      " --> Best model saved at epoch 19\n",
      "Epoch 20/100 | Train Loss: 0.7931 | Val Loss: 3.2530 | Val RMSE: 1.8036\n",
      "Epoch 21/100 | Train Loss: 0.7896 | Val Loss: 3.2497 | Val RMSE: 1.8027\n",
      "Epoch 22/100 | Train Loss: 0.7864 | Val Loss: 3.2449 | Val RMSE: 1.8014\n",
      " --> Best model saved at epoch 22\n",
      "Epoch 23/100 | Train Loss: 0.7826 | Val Loss: 3.2493 | Val RMSE: 1.8026\n",
      "Epoch 24/100 | Train Loss: 0.7787 | Val Loss: 3.2474 | Val RMSE: 1.8021\n",
      "Epoch 25/100 | Train Loss: 0.7768 | Val Loss: 3.2507 | Val RMSE: 1.8030\n",
      "Epoch 26/100 | Train Loss: 0.7733 | Val Loss: 3.2511 | Val RMSE: 1.8031\n",
      "Epoch 27/100 | Train Loss: 0.7701 | Val Loss: 3.2472 | Val RMSE: 1.8020\n",
      "Epoch 28/100 | Train Loss: 0.7694 | Val Loss: 3.2436 | Val RMSE: 1.8010\n",
      " --> Best model saved at epoch 28\n",
      "Epoch 29/100 | Train Loss: 0.7653 | Val Loss: 3.2473 | Val RMSE: 1.8020\n",
      "Epoch 30/100 | Train Loss: 0.7645 | Val Loss: 3.2432 | Val RMSE: 1.8009\n",
      " --> Best model saved at epoch 30\n",
      "Epoch 31/100 | Train Loss: 0.7602 | Val Loss: 3.2471 | Val RMSE: 1.8020\n",
      "Epoch 32/100 | Train Loss: 0.7570 | Val Loss: 3.2465 | Val RMSE: 1.8018\n",
      "Epoch 33/100 | Train Loss: 0.7556 | Val Loss: 3.2485 | Val RMSE: 1.8024\n",
      "Epoch 34/100 | Train Loss: 0.7543 | Val Loss: 3.2488 | Val RMSE: 1.8024\n",
      "Epoch 35/100 | Train Loss: 0.7516 | Val Loss: 3.2484 | Val RMSE: 1.8023\n",
      "Epoch 36/100 | Train Loss: 0.7495 | Val Loss: 3.2477 | Val RMSE: 1.8021\n",
      "Epoch 37/100 | Train Loss: 0.7455 | Val Loss: 3.2532 | Val RMSE: 1.8037\n",
      "Epoch 38/100 | Train Loss: 0.7439 | Val Loss: 3.2512 | Val RMSE: 1.8031\n",
      "Epoch 39/100 | Train Loss: 0.7424 | Val Loss: 3.2471 | Val RMSE: 1.8020\n",
      "Epoch 40/100 | Train Loss: 0.7398 | Val Loss: 3.2495 | Val RMSE: 1.8026\n",
      "Epoch 41/100 | Train Loss: 0.7378 | Val Loss: 3.2514 | Val RMSE: 1.8032\n",
      "Epoch 42/100 | Train Loss: 0.7361 | Val Loss: 3.2535 | Val RMSE: 1.8038\n",
      "Epoch 43/100 | Train Loss: 0.7350 | Val Loss: 3.2510 | Val RMSE: 1.8031\n",
      "Epoch 44/100 | Train Loss: 0.7321 | Val Loss: 3.2479 | Val RMSE: 1.8022\n",
      "Epoch 45/100 | Train Loss: 0.7303 | Val Loss: 3.2489 | Val RMSE: 1.8025\n",
      "Epoch 46/100 | Train Loss: 0.7287 | Val Loss: 3.2459 | Val RMSE: 1.8017\n",
      "Epoch 47/100 | Train Loss: 0.7353 | Val Loss: 3.2403 | Val RMSE: 1.8001\n",
      " --> Best model saved at epoch 47\n",
      "Epoch 48/100 | Train Loss: 0.7261 | Val Loss: 3.2523 | Val RMSE: 1.8034\n",
      "Epoch 49/100 | Train Loss: 0.7230 | Val Loss: 3.2463 | Val RMSE: 1.8017\n",
      "Epoch 50/100 | Train Loss: 0.7223 | Val Loss: 3.2493 | Val RMSE: 1.8026\n",
      "Epoch 51/100 | Train Loss: 0.7190 | Val Loss: 3.2501 | Val RMSE: 1.8028\n",
      "Epoch 52/100 | Train Loss: 0.7178 | Val Loss: 3.2515 | Val RMSE: 1.8032\n",
      "Epoch 53/100 | Train Loss: 0.7200 | Val Loss: 3.2459 | Val RMSE: 1.8016\n",
      "Epoch 54/100 | Train Loss: 0.7162 | Val Loss: 3.2427 | Val RMSE: 1.8008\n",
      "Epoch 55/100 | Train Loss: 0.7135 | Val Loss: 3.2495 | Val RMSE: 1.8026\n",
      "Epoch 56/100 | Train Loss: 0.7126 | Val Loss: 3.2499 | Val RMSE: 1.8027\n",
      "Epoch 57/100 | Train Loss: 0.7094 | Val Loss: 3.2560 | Val RMSE: 1.8044\n",
      "Epoch 58/100 | Train Loss: 0.7089 | Val Loss: 3.2526 | Val RMSE: 1.8035\n",
      "Epoch 59/100 | Train Loss: 0.7060 | Val Loss: 3.2505 | Val RMSE: 1.8029\n",
      "Epoch 60/100 | Train Loss: 0.7040 | Val Loss: 3.2575 | Val RMSE: 1.8049\n",
      "Epoch 61/100 | Train Loss: 0.7023 | Val Loss: 3.2493 | Val RMSE: 1.8026\n",
      "Epoch 62/100 | Train Loss: 0.7001 | Val Loss: 3.2467 | Val RMSE: 1.8019\n",
      "Epoch 63/100 | Train Loss: 0.6986 | Val Loss: 3.2474 | Val RMSE: 1.8020\n",
      "Epoch 64/100 | Train Loss: 0.6966 | Val Loss: 3.2511 | Val RMSE: 1.8031\n",
      "Epoch 65/100 | Train Loss: 0.6968 | Val Loss: 3.2484 | Val RMSE: 1.8023\n",
      "Epoch 66/100 | Train Loss: 0.6945 | Val Loss: 3.2432 | Val RMSE: 1.8009\n",
      "Epoch 67/100 | Train Loss: 0.6993 | Val Loss: 3.2482 | Val RMSE: 1.8023\n",
      "Early stopping triggered.\n",
      "Epoch 1/100 | Train Loss: 1.0040 | Val Loss: 0.8647 | Val RMSE: 0.9299\n",
      " --> Best model saved at epoch 1\n",
      "Epoch 2/100 | Train Loss: 0.9995 | Val Loss: 0.8636 | Val RMSE: 0.9293\n",
      " --> Best model saved at epoch 2\n",
      "Epoch 3/100 | Train Loss: 0.9966 | Val Loss: 0.8546 | Val RMSE: 0.9244\n",
      " --> Best model saved at epoch 3\n",
      "Epoch 4/100 | Train Loss: 0.9931 | Val Loss: 0.8506 | Val RMSE: 0.9223\n",
      " --> Best model saved at epoch 4\n",
      "Epoch 5/100 | Train Loss: 0.9892 | Val Loss: 0.8466 | Val RMSE: 0.9201\n",
      " --> Best model saved at epoch 5\n",
      "Epoch 6/100 | Train Loss: 0.9827 | Val Loss: 0.8535 | Val RMSE: 0.9239\n",
      "Epoch 7/100 | Train Loss: 0.9730 | Val Loss: 0.8402 | Val RMSE: 0.9166\n",
      " --> Best model saved at epoch 7\n",
      "Epoch 8/100 | Train Loss: 0.9568 | Val Loss: 0.8443 | Val RMSE: 0.9188\n",
      "Epoch 9/100 | Train Loss: 0.9323 | Val Loss: 0.8158 | Val RMSE: 0.9032\n",
      " --> Best model saved at epoch 9\n",
      "Epoch 10/100 | Train Loss: 0.9112 | Val Loss: 0.8260 | Val RMSE: 0.9089\n",
      "Epoch 11/100 | Train Loss: 0.8939 | Val Loss: 0.8200 | Val RMSE: 0.9055\n",
      "Epoch 12/100 | Train Loss: 0.8806 | Val Loss: 0.8033 | Val RMSE: 0.8963\n",
      " --> Best model saved at epoch 12\n",
      "Epoch 13/100 | Train Loss: 0.8706 | Val Loss: 0.8241 | Val RMSE: 0.9078\n",
      "Epoch 14/100 | Train Loss: 0.8611 | Val Loss: 0.7801 | Val RMSE: 0.8832\n",
      " --> Best model saved at epoch 14\n",
      "Epoch 15/100 | Train Loss: 0.8470 | Val Loss: 0.7605 | Val RMSE: 0.8721\n",
      " --> Best model saved at epoch 15\n",
      "Epoch 16/100 | Train Loss: 0.8385 | Val Loss: 0.7859 | Val RMSE: 0.8865\n",
      "Epoch 17/100 | Train Loss: 0.8297 | Val Loss: 0.7479 | Val RMSE: 0.8648\n",
      " --> Best model saved at epoch 17\n",
      "Epoch 18/100 | Train Loss: 0.8173 | Val Loss: 0.7512 | Val RMSE: 0.8667\n",
      "Epoch 19/100 | Train Loss: 0.8133 | Val Loss: 0.7164 | Val RMSE: 0.8464\n",
      " --> Best model saved at epoch 19\n",
      "Epoch 20/100 | Train Loss: 0.7995 | Val Loss: 0.7227 | Val RMSE: 0.8501\n",
      "Epoch 21/100 | Train Loss: 0.7912 | Val Loss: 0.7202 | Val RMSE: 0.8486\n",
      "Epoch 22/100 | Train Loss: 0.7858 | Val Loss: 0.7209 | Val RMSE: 0.8490\n",
      "Epoch 23/100 | Train Loss: 0.7788 | Val Loss: 0.7121 | Val RMSE: 0.8438\n",
      " --> Best model saved at epoch 23\n",
      "Epoch 24/100 | Train Loss: 0.7722 | Val Loss: 0.7107 | Val RMSE: 0.8430\n",
      " --> Best model saved at epoch 24\n",
      "Epoch 25/100 | Train Loss: 0.7677 | Val Loss: 0.7322 | Val RMSE: 0.8557\n",
      "Epoch 26/100 | Train Loss: 0.7694 | Val Loss: 0.7050 | Val RMSE: 0.8396\n",
      " --> Best model saved at epoch 26\n",
      "Epoch 27/100 | Train Loss: 0.7692 | Val Loss: 0.7116 | Val RMSE: 0.8436\n",
      "Epoch 28/100 | Train Loss: 0.7636 | Val Loss: 0.7050 | Val RMSE: 0.8397\n",
      "Epoch 29/100 | Train Loss: 0.7625 | Val Loss: 0.7056 | Val RMSE: 0.8400\n",
      "Epoch 30/100 | Train Loss: 0.7564 | Val Loss: 0.7217 | Val RMSE: 0.8495\n",
      "Epoch 31/100 | Train Loss: 0.7570 | Val Loss: 0.7091 | Val RMSE: 0.8421\n",
      "Epoch 32/100 | Train Loss: 0.7589 | Val Loss: 0.7049 | Val RMSE: 0.8396\n",
      " --> Best model saved at epoch 32\n",
      "Epoch 33/100 | Train Loss: 0.7497 | Val Loss: 0.7203 | Val RMSE: 0.8487\n",
      "Epoch 34/100 | Train Loss: 0.7486 | Val Loss: 0.7232 | Val RMSE: 0.8504\n",
      "Epoch 35/100 | Train Loss: 0.7498 | Val Loss: 0.7150 | Val RMSE: 0.8456\n",
      "Epoch 36/100 | Train Loss: 0.7503 | Val Loss: 0.7165 | Val RMSE: 0.8464\n",
      "Epoch 37/100 | Train Loss: 0.7495 | Val Loss: 0.6991 | Val RMSE: 0.8361\n",
      " --> Best model saved at epoch 37\n",
      "Epoch 38/100 | Train Loss: 0.7430 | Val Loss: 0.7081 | Val RMSE: 0.8415\n",
      "Epoch 39/100 | Train Loss: 0.7451 | Val Loss: 0.7032 | Val RMSE: 0.8386\n",
      "Epoch 40/100 | Train Loss: 0.7429 | Val Loss: 0.7081 | Val RMSE: 0.8415\n",
      "Epoch 41/100 | Train Loss: 0.7397 | Val Loss: 0.7019 | Val RMSE: 0.8378\n",
      "Epoch 42/100 | Train Loss: 0.7376 | Val Loss: 0.7182 | Val RMSE: 0.8475\n",
      "Epoch 43/100 | Train Loss: 0.7357 | Val Loss: 0.7148 | Val RMSE: 0.8454\n",
      "Epoch 44/100 | Train Loss: 0.7319 | Val Loss: 0.7091 | Val RMSE: 0.8421\n",
      "Epoch 45/100 | Train Loss: 0.7360 | Val Loss: 0.7084 | Val RMSE: 0.8417\n",
      "Epoch 46/100 | Train Loss: 0.7302 | Val Loss: 0.7133 | Val RMSE: 0.8446\n",
      "Epoch 47/100 | Train Loss: 0.7310 | Val Loss: 0.7217 | Val RMSE: 0.8495\n",
      "Epoch 48/100 | Train Loss: 0.7306 | Val Loss: 0.7175 | Val RMSE: 0.8470\n",
      "Epoch 49/100 | Train Loss: 0.7282 | Val Loss: 0.7261 | Val RMSE: 0.8521\n",
      "Epoch 50/100 | Train Loss: 0.7281 | Val Loss: 0.7090 | Val RMSE: 0.8420\n",
      "Epoch 51/100 | Train Loss: 0.7258 | Val Loss: 0.7293 | Val RMSE: 0.8540\n",
      "Epoch 52/100 | Train Loss: 0.7257 | Val Loss: 0.7086 | Val RMSE: 0.8418\n",
      "Epoch 53/100 | Train Loss: 0.7221 | Val Loss: 0.7152 | Val RMSE: 0.8457\n",
      "Epoch 54/100 | Train Loss: 0.7327 | Val Loss: 0.7041 | Val RMSE: 0.8391\n",
      "Epoch 55/100 | Train Loss: 0.7228 | Val Loss: 0.7022 | Val RMSE: 0.8380\n",
      "Epoch 56/100 | Train Loss: 0.7209 | Val Loss: 0.7119 | Val RMSE: 0.8437\n",
      "Epoch 57/100 | Train Loss: 0.7197 | Val Loss: 0.7183 | Val RMSE: 0.8475\n",
      "Early stopping triggered.\n",
      "Epoch 1/100 | Train Loss: 1.0584 | Val Loss: 0.6791 | Val RMSE: 0.8241\n",
      " --> Best model saved at epoch 1\n",
      "Epoch 2/100 | Train Loss: 1.0327 | Val Loss: 0.6798 | Val RMSE: 0.8245\n",
      "Epoch 3/100 | Train Loss: 1.0275 | Val Loss: 0.6872 | Val RMSE: 0.8290\n",
      "Epoch 4/100 | Train Loss: 1.0241 | Val Loss: 0.6787 | Val RMSE: 0.8238\n",
      " --> Best model saved at epoch 4\n",
      "Epoch 5/100 | Train Loss: 1.0192 | Val Loss: 0.6793 | Val RMSE: 0.8242\n",
      "Epoch 6/100 | Train Loss: 1.0166 | Val Loss: 0.6712 | Val RMSE: 0.8193\n",
      " --> Best model saved at epoch 6\n",
      "Epoch 7/100 | Train Loss: 1.0081 | Val Loss: 0.6702 | Val RMSE: 0.8187\n",
      " --> Best model saved at epoch 7\n",
      "Epoch 8/100 | Train Loss: 0.9974 | Val Loss: 0.6751 | Val RMSE: 0.8216\n",
      "Epoch 9/100 | Train Loss: 0.9832 | Val Loss: 0.6689 | Val RMSE: 0.8178\n",
      " --> Best model saved at epoch 9\n",
      "Epoch 10/100 | Train Loss: 0.9699 | Val Loss: 0.6476 | Val RMSE: 0.8048\n",
      " --> Best model saved at epoch 10\n",
      "Epoch 11/100 | Train Loss: 0.9514 | Val Loss: 0.6621 | Val RMSE: 0.8137\n",
      "Epoch 12/100 | Train Loss: 0.9305 | Val Loss: 0.6603 | Val RMSE: 0.8126\n",
      "Epoch 13/100 | Train Loss: 0.9052 | Val Loss: 0.6384 | Val RMSE: 0.7990\n",
      " --> Best model saved at epoch 13\n",
      "Epoch 14/100 | Train Loss: 0.8882 | Val Loss: 0.6216 | Val RMSE: 0.7884\n",
      " --> Best model saved at epoch 14\n",
      "Epoch 15/100 | Train Loss: 0.8744 | Val Loss: 0.6429 | Val RMSE: 0.8018\n",
      "Epoch 16/100 | Train Loss: 0.8636 | Val Loss: 0.6305 | Val RMSE: 0.7940\n",
      "Epoch 17/100 | Train Loss: 0.8565 | Val Loss: 0.6290 | Val RMSE: 0.7931\n",
      "Epoch 18/100 | Train Loss: 0.8513 | Val Loss: 0.6184 | Val RMSE: 0.7864\n",
      " --> Best model saved at epoch 18\n",
      "Epoch 19/100 | Train Loss: 0.8457 | Val Loss: 0.6210 | Val RMSE: 0.7881\n",
      "Epoch 20/100 | Train Loss: 0.8420 | Val Loss: 0.6211 | Val RMSE: 0.7881\n",
      "Epoch 21/100 | Train Loss: 0.8390 | Val Loss: 0.6140 | Val RMSE: 0.7836\n",
      " --> Best model saved at epoch 21\n",
      "Epoch 22/100 | Train Loss: 0.8346 | Val Loss: 0.6137 | Val RMSE: 0.7834\n",
      " --> Best model saved at epoch 22\n",
      "Epoch 23/100 | Train Loss: 0.8296 | Val Loss: 0.6160 | Val RMSE: 0.7848\n",
      "Epoch 24/100 | Train Loss: 0.8271 | Val Loss: 0.6232 | Val RMSE: 0.7894\n",
      "Epoch 25/100 | Train Loss: 0.8233 | Val Loss: 0.6159 | Val RMSE: 0.7848\n",
      "Epoch 26/100 | Train Loss: 0.8238 | Val Loss: 0.6149 | Val RMSE: 0.7841\n",
      "Epoch 27/100 | Train Loss: 0.8187 | Val Loss: 0.6062 | Val RMSE: 0.7786\n",
      " --> Best model saved at epoch 27\n",
      "Epoch 28/100 | Train Loss: 0.8141 | Val Loss: 0.6086 | Val RMSE: 0.7801\n",
      "Epoch 29/100 | Train Loss: 0.8130 | Val Loss: 0.6098 | Val RMSE: 0.7809\n",
      "Epoch 30/100 | Train Loss: 0.8086 | Val Loss: 0.6194 | Val RMSE: 0.7870\n",
      "Epoch 31/100 | Train Loss: 0.8056 | Val Loss: 0.6228 | Val RMSE: 0.7892\n",
      "Epoch 32/100 | Train Loss: 0.8022 | Val Loss: 0.6048 | Val RMSE: 0.7777\n",
      " --> Best model saved at epoch 32\n",
      "Epoch 33/100 | Train Loss: 0.7991 | Val Loss: 0.6010 | Val RMSE: 0.7752\n",
      " --> Best model saved at epoch 33\n",
      "Epoch 34/100 | Train Loss: 0.7934 | Val Loss: 0.5921 | Val RMSE: 0.7695\n",
      " --> Best model saved at epoch 34\n",
      "Epoch 35/100 | Train Loss: 0.7942 | Val Loss: 0.5915 | Val RMSE: 0.7691\n",
      " --> Best model saved at epoch 35\n",
      "Epoch 36/100 | Train Loss: 0.7894 | Val Loss: 0.5973 | Val RMSE: 0.7729\n",
      "Epoch 37/100 | Train Loss: 0.7883 | Val Loss: 0.5892 | Val RMSE: 0.7676\n",
      " --> Best model saved at epoch 37\n",
      "Epoch 38/100 | Train Loss: 0.7842 | Val Loss: 0.5999 | Val RMSE: 0.7745\n",
      "Epoch 39/100 | Train Loss: 0.7803 | Val Loss: 0.5901 | Val RMSE: 0.7682\n",
      "Epoch 40/100 | Train Loss: 0.7766 | Val Loss: 0.5918 | Val RMSE: 0.7693\n",
      "Epoch 41/100 | Train Loss: 0.7762 | Val Loss: 0.5905 | Val RMSE: 0.7685\n",
      "Epoch 42/100 | Train Loss: 0.7737 | Val Loss: 0.5996 | Val RMSE: 0.7743\n",
      "Epoch 43/100 | Train Loss: 0.7698 | Val Loss: 0.6053 | Val RMSE: 0.7780\n",
      "Epoch 44/100 | Train Loss: 0.7704 | Val Loss: 0.5909 | Val RMSE: 0.7687\n",
      "Epoch 45/100 | Train Loss: 0.7658 | Val Loss: 0.5906 | Val RMSE: 0.7685\n",
      "Epoch 46/100 | Train Loss: 0.7647 | Val Loss: 0.5834 | Val RMSE: 0.7638\n",
      " --> Best model saved at epoch 46\n",
      "Epoch 47/100 | Train Loss: 0.7601 | Val Loss: 0.6079 | Val RMSE: 0.7797\n",
      "Epoch 48/100 | Train Loss: 0.7614 | Val Loss: 0.5881 | Val RMSE: 0.7669\n",
      "Epoch 49/100 | Train Loss: 0.7574 | Val Loss: 0.6099 | Val RMSE: 0.7810\n",
      "Epoch 50/100 | Train Loss: 0.7589 | Val Loss: 0.6062 | Val RMSE: 0.7786\n",
      "Epoch 51/100 | Train Loss: 0.7569 | Val Loss: 0.5923 | Val RMSE: 0.7696\n",
      "Epoch 52/100 | Train Loss: 0.7545 | Val Loss: 0.5950 | Val RMSE: 0.7714\n",
      "Epoch 53/100 | Train Loss: 0.7543 | Val Loss: 0.5941 | Val RMSE: 0.7708\n",
      "Epoch 54/100 | Train Loss: 0.7517 | Val Loss: 0.5955 | Val RMSE: 0.7717\n",
      "Epoch 55/100 | Train Loss: 0.7490 | Val Loss: 0.5886 | Val RMSE: 0.7672\n",
      "Epoch 56/100 | Train Loss: 0.7463 | Val Loss: 0.5922 | Val RMSE: 0.7695\n",
      "Epoch 57/100 | Train Loss: 0.7449 | Val Loss: 0.6083 | Val RMSE: 0.7800\n",
      "Epoch 58/100 | Train Loss: 0.7428 | Val Loss: 0.5807 | Val RMSE: 0.7620\n",
      " --> Best model saved at epoch 58\n",
      "Epoch 59/100 | Train Loss: 0.7428 | Val Loss: 0.6043 | Val RMSE: 0.7774\n",
      "Epoch 60/100 | Train Loss: 0.7414 | Val Loss: 0.5934 | Val RMSE: 0.7703\n",
      "Epoch 61/100 | Train Loss: 0.7396 | Val Loss: 0.5983 | Val RMSE: 0.7735\n",
      "Epoch 62/100 | Train Loss: 0.7426 | Val Loss: 0.5851 | Val RMSE: 0.7649\n",
      "Epoch 63/100 | Train Loss: 0.7379 | Val Loss: 0.6050 | Val RMSE: 0.7778\n",
      "Epoch 64/100 | Train Loss: 0.7358 | Val Loss: 0.5884 | Val RMSE: 0.7671\n",
      "Epoch 65/100 | Train Loss: 0.7366 | Val Loss: 0.6239 | Val RMSE: 0.7899\n",
      "Epoch 66/100 | Train Loss: 0.7342 | Val Loss: 0.5984 | Val RMSE: 0.7735\n",
      "Epoch 67/100 | Train Loss: 0.7334 | Val Loss: 0.6119 | Val RMSE: 0.7822\n",
      "Epoch 68/100 | Train Loss: 0.7334 | Val Loss: 0.6143 | Val RMSE: 0.7838\n",
      "Epoch 69/100 | Train Loss: 0.7310 | Val Loss: 0.6084 | Val RMSE: 0.7800\n",
      "Epoch 70/100 | Train Loss: 0.7299 | Val Loss: 0.6173 | Val RMSE: 0.7857\n",
      "Epoch 71/100 | Train Loss: 0.7281 | Val Loss: 0.6176 | Val RMSE: 0.7859\n",
      "Epoch 72/100 | Train Loss: 0.7264 | Val Loss: 0.6101 | Val RMSE: 0.7811\n",
      "Epoch 73/100 | Train Loss: 0.7256 | Val Loss: 0.6042 | Val RMSE: 0.7773\n",
      "Epoch 74/100 | Train Loss: 0.7250 | Val Loss: 0.6230 | Val RMSE: 0.7893\n",
      "Epoch 75/100 | Train Loss: 0.7250 | Val Loss: 0.6127 | Val RMSE: 0.7828\n",
      "Epoch 76/100 | Train Loss: 0.7215 | Val Loss: 0.6075 | Val RMSE: 0.7794\n",
      "Epoch 77/100 | Train Loss: 0.7204 | Val Loss: 0.6097 | Val RMSE: 0.7808\n",
      "Epoch 78/100 | Train Loss: 0.7194 | Val Loss: 0.6150 | Val RMSE: 0.7842\n",
      "Early stopping triggered.\n",
      "Epoch 1/100 | Train Loss: 1.0186 | Val Loss: 0.6460 | Val RMSE: 0.8038\n",
      " --> Best model saved at epoch 1\n",
      "Epoch 2/100 | Train Loss: 1.0082 | Val Loss: 0.6472 | Val RMSE: 0.8045\n",
      "Epoch 3/100 | Train Loss: 1.0046 | Val Loss: 0.6515 | Val RMSE: 0.8071\n",
      "Epoch 4/100 | Train Loss: 1.0019 | Val Loss: 0.6517 | Val RMSE: 0.8073\n",
      "Epoch 5/100 | Train Loss: 0.9980 | Val Loss: 0.6530 | Val RMSE: 0.8081\n",
      "Epoch 6/100 | Train Loss: 0.9924 | Val Loss: 0.6532 | Val RMSE: 0.8082\n",
      "Epoch 7/100 | Train Loss: 0.9849 | Val Loss: 0.6573 | Val RMSE: 0.8107\n",
      "Epoch 8/100 | Train Loss: 0.9743 | Val Loss: 0.6554 | Val RMSE: 0.8095\n",
      "Epoch 9/100 | Train Loss: 0.9580 | Val Loss: 0.6478 | Val RMSE: 0.8048\n",
      "Epoch 10/100 | Train Loss: 0.9387 | Val Loss: 0.6325 | Val RMSE: 0.7953\n",
      " --> Best model saved at epoch 10\n",
      "Epoch 11/100 | Train Loss: 0.9229 | Val Loss: 0.6205 | Val RMSE: 0.7877\n",
      " --> Best model saved at epoch 11\n",
      "Epoch 12/100 | Train Loss: 0.9087 | Val Loss: 0.6061 | Val RMSE: 0.7785\n",
      " --> Best model saved at epoch 12\n",
      "Epoch 13/100 | Train Loss: 0.8957 | Val Loss: 0.6047 | Val RMSE: 0.7776\n",
      " --> Best model saved at epoch 13\n",
      "Epoch 14/100 | Train Loss: 0.8859 | Val Loss: 0.5954 | Val RMSE: 0.7716\n",
      " --> Best model saved at epoch 14\n",
      "Epoch 15/100 | Train Loss: 0.8740 | Val Loss: 0.5932 | Val RMSE: 0.7702\n",
      " --> Best model saved at epoch 15\n",
      "Epoch 16/100 | Train Loss: 0.8656 | Val Loss: 0.5781 | Val RMSE: 0.7603\n",
      " --> Best model saved at epoch 16\n",
      "Epoch 17/100 | Train Loss: 0.8557 | Val Loss: 0.5741 | Val RMSE: 0.7577\n",
      " --> Best model saved at epoch 17\n",
      "Epoch 18/100 | Train Loss: 0.8463 | Val Loss: 0.5701 | Val RMSE: 0.7551\n",
      " --> Best model saved at epoch 18\n",
      "Epoch 19/100 | Train Loss: 0.8366 | Val Loss: 0.5616 | Val RMSE: 0.7494\n",
      " --> Best model saved at epoch 19\n",
      "Epoch 20/100 | Train Loss: 0.8313 | Val Loss: 0.5593 | Val RMSE: 0.7479\n",
      " --> Best model saved at epoch 20\n",
      "Epoch 21/100 | Train Loss: 0.8238 | Val Loss: 0.5467 | Val RMSE: 0.7394\n",
      " --> Best model saved at epoch 21\n",
      "Epoch 22/100 | Train Loss: 0.8170 | Val Loss: 0.5466 | Val RMSE: 0.7393\n",
      " --> Best model saved at epoch 22\n",
      "Epoch 23/100 | Train Loss: 0.8094 | Val Loss: 0.5480 | Val RMSE: 0.7403\n",
      "Epoch 24/100 | Train Loss: 0.8055 | Val Loss: 0.5325 | Val RMSE: 0.7297\n",
      " --> Best model saved at epoch 24\n",
      "Epoch 25/100 | Train Loss: 0.7988 | Val Loss: 0.5409 | Val RMSE: 0.7355\n",
      "Epoch 26/100 | Train Loss: 0.7955 | Val Loss: 0.5400 | Val RMSE: 0.7348\n",
      "Epoch 27/100 | Train Loss: 0.7914 | Val Loss: 0.5366 | Val RMSE: 0.7325\n",
      "Epoch 28/100 | Train Loss: 0.7865 | Val Loss: 0.5383 | Val RMSE: 0.7337\n",
      "Epoch 29/100 | Train Loss: 0.7833 | Val Loss: 0.5422 | Val RMSE: 0.7364\n",
      "Epoch 30/100 | Train Loss: 0.7794 | Val Loss: 0.5388 | Val RMSE: 0.7340\n",
      "Epoch 31/100 | Train Loss: 0.7754 | Val Loss: 0.5409 | Val RMSE: 0.7354\n",
      "Epoch 32/100 | Train Loss: 0.7716 | Val Loss: 0.5419 | Val RMSE: 0.7361\n",
      "Epoch 33/100 | Train Loss: 0.7697 | Val Loss: 0.5433 | Val RMSE: 0.7371\n",
      "Epoch 34/100 | Train Loss: 0.7645 | Val Loss: 0.5420 | Val RMSE: 0.7362\n",
      "Epoch 35/100 | Train Loss: 0.7620 | Val Loss: 0.5431 | Val RMSE: 0.7369\n",
      "Epoch 36/100 | Train Loss: 0.7589 | Val Loss: 0.5456 | Val RMSE: 0.7386\n",
      "Epoch 37/100 | Train Loss: 0.7567 | Val Loss: 0.5455 | Val RMSE: 0.7386\n",
      "Epoch 38/100 | Train Loss: 0.7534 | Val Loss: 0.5348 | Val RMSE: 0.7313\n",
      "Epoch 39/100 | Train Loss: 0.7493 | Val Loss: 0.5447 | Val RMSE: 0.7381\n",
      "Epoch 40/100 | Train Loss: 0.7468 | Val Loss: 0.5495 | Val RMSE: 0.7413\n",
      "Epoch 41/100 | Train Loss: 0.7460 | Val Loss: 0.5444 | Val RMSE: 0.7378\n",
      "Epoch 42/100 | Train Loss: 0.7414 | Val Loss: 0.5409 | Val RMSE: 0.7355\n",
      "Epoch 43/100 | Train Loss: 0.7397 | Val Loss: 0.5369 | Val RMSE: 0.7327\n",
      "Epoch 44/100 | Train Loss: 0.7389 | Val Loss: 0.5403 | Val RMSE: 0.7351\n",
      "Early stopping triggered.\n",
      "Epoch 1/100 | Train Loss: 0.9932 | Val Loss: 5.5161 | Val RMSE: 2.3486\n",
      " --> Best model saved at epoch 1\n",
      "Epoch 2/100 | Train Loss: 0.9772 | Val Loss: 5.5076 | Val RMSE: 2.3468\n",
      " --> Best model saved at epoch 2\n",
      "Epoch 3/100 | Train Loss: 0.9743 | Val Loss: 5.5018 | Val RMSE: 2.3456\n",
      " --> Best model saved at epoch 3\n",
      "Epoch 4/100 | Train Loss: 0.9707 | Val Loss: 5.4930 | Val RMSE: 2.3437\n",
      " --> Best model saved at epoch 4\n",
      "Epoch 5/100 | Train Loss: 0.9670 | Val Loss: 5.4810 | Val RMSE: 2.3411\n",
      " --> Best model saved at epoch 5\n",
      "Epoch 6/100 | Train Loss: 0.9584 | Val Loss: 5.4552 | Val RMSE: 2.3356\n",
      " --> Best model saved at epoch 6\n",
      "Epoch 7/100 | Train Loss: 0.9454 | Val Loss: 5.4168 | Val RMSE: 2.3274\n",
      " --> Best model saved at epoch 7\n",
      "Epoch 8/100 | Train Loss: 0.9219 | Val Loss: 5.4021 | Val RMSE: 2.3242\n",
      " --> Best model saved at epoch 8\n",
      "Epoch 9/100 | Train Loss: 0.8850 | Val Loss: 5.3957 | Val RMSE: 2.3229\n",
      " --> Best model saved at epoch 9\n",
      "Epoch 10/100 | Train Loss: 0.8595 | Val Loss: 5.4054 | Val RMSE: 2.3250\n",
      "Epoch 11/100 | Train Loss: 0.8410 | Val Loss: 5.3990 | Val RMSE: 2.3236\n",
      "Epoch 12/100 | Train Loss: 0.8260 | Val Loss: 5.3870 | Val RMSE: 2.3210\n",
      " --> Best model saved at epoch 12\n",
      "Epoch 13/100 | Train Loss: 0.8107 | Val Loss: 5.3757 | Val RMSE: 2.3186\n",
      " --> Best model saved at epoch 13\n",
      "Epoch 14/100 | Train Loss: 0.7986 | Val Loss: 5.3714 | Val RMSE: 2.3176\n",
      " --> Best model saved at epoch 14\n",
      "Epoch 15/100 | Train Loss: 0.7884 | Val Loss: 5.3673 | Val RMSE: 2.3167\n",
      " --> Best model saved at epoch 15\n",
      "Epoch 16/100 | Train Loss: 0.7813 | Val Loss: 5.3716 | Val RMSE: 2.3177\n",
      "Epoch 17/100 | Train Loss: 0.7730 | Val Loss: 5.3554 | Val RMSE: 2.3142\n",
      " --> Best model saved at epoch 17\n",
      "Epoch 18/100 | Train Loss: 0.7678 | Val Loss: 5.3616 | Val RMSE: 2.3155\n",
      "Epoch 19/100 | Train Loss: 0.7635 | Val Loss: 5.3402 | Val RMSE: 2.3109\n",
      " --> Best model saved at epoch 19\n",
      "Epoch 20/100 | Train Loss: 0.7576 | Val Loss: 5.3612 | Val RMSE: 2.3154\n",
      "Epoch 21/100 | Train Loss: 0.7506 | Val Loss: 5.3415 | Val RMSE: 2.3112\n",
      "Epoch 22/100 | Train Loss: 0.7468 | Val Loss: 5.3383 | Val RMSE: 2.3105\n",
      " --> Best model saved at epoch 22\n",
      "Epoch 23/100 | Train Loss: 0.7448 | Val Loss: 5.3257 | Val RMSE: 2.3077\n",
      " --> Best model saved at epoch 23\n",
      "Epoch 24/100 | Train Loss: 0.7399 | Val Loss: 5.3244 | Val RMSE: 2.3075\n",
      " --> Best model saved at epoch 24\n",
      "Epoch 25/100 | Train Loss: 0.7373 | Val Loss: 5.3406 | Val RMSE: 2.3110\n",
      "Epoch 26/100 | Train Loss: 0.7329 | Val Loss: 5.3359 | Val RMSE: 2.3100\n",
      "Epoch 27/100 | Train Loss: 0.7278 | Val Loss: 5.3252 | Val RMSE: 2.3076\n",
      "Epoch 28/100 | Train Loss: 0.7252 | Val Loss: 5.3297 | Val RMSE: 2.3086\n",
      "Epoch 29/100 | Train Loss: 0.7224 | Val Loss: 5.3268 | Val RMSE: 2.3080\n",
      "Epoch 30/100 | Train Loss: 0.7196 | Val Loss: 5.3432 | Val RMSE: 2.3115\n",
      "Epoch 31/100 | Train Loss: 0.7149 | Val Loss: 5.3353 | Val RMSE: 2.3098\n",
      "Epoch 32/100 | Train Loss: 0.7108 | Val Loss: 5.3187 | Val RMSE: 2.3062\n",
      " --> Best model saved at epoch 32\n",
      "Epoch 33/100 | Train Loss: 0.7081 | Val Loss: 5.3396 | Val RMSE: 2.3108\n",
      "Epoch 34/100 | Train Loss: 0.7064 | Val Loss: 5.3390 | Val RMSE: 2.3106\n",
      "Epoch 35/100 | Train Loss: 0.7026 | Val Loss: 5.3400 | Val RMSE: 2.3108\n",
      "Epoch 36/100 | Train Loss: 0.7011 | Val Loss: 5.3221 | Val RMSE: 2.3070\n",
      "Epoch 37/100 | Train Loss: 0.6989 | Val Loss: 5.3280 | Val RMSE: 2.3082\n",
      "Epoch 38/100 | Train Loss: 0.6948 | Val Loss: 5.3463 | Val RMSE: 2.3122\n",
      "Epoch 39/100 | Train Loss: 0.6922 | Val Loss: 5.3504 | Val RMSE: 2.3131\n",
      "Epoch 40/100 | Train Loss: 0.6915 | Val Loss: 5.3392 | Val RMSE: 2.3107\n",
      "Epoch 41/100 | Train Loss: 0.6882 | Val Loss: 5.3351 | Val RMSE: 2.3098\n",
      "Epoch 42/100 | Train Loss: 0.6855 | Val Loss: 5.3635 | Val RMSE: 2.3159\n",
      "Epoch 43/100 | Train Loss: 0.6832 | Val Loss: 5.3528 | Val RMSE: 2.3136\n",
      "Epoch 44/100 | Train Loss: 0.6827 | Val Loss: 5.3663 | Val RMSE: 2.3165\n",
      "Epoch 45/100 | Train Loss: 0.6793 | Val Loss: 5.3589 | Val RMSE: 2.3149\n",
      "Epoch 46/100 | Train Loss: 0.6793 | Val Loss: 5.3642 | Val RMSE: 2.3161\n",
      "Epoch 47/100 | Train Loss: 0.6758 | Val Loss: 5.3705 | Val RMSE: 2.3174\n",
      "Epoch 48/100 | Train Loss: 0.6767 | Val Loss: 5.3779 | Val RMSE: 2.3190\n",
      "Epoch 49/100 | Train Loss: 0.6726 | Val Loss: 5.3652 | Val RMSE: 2.3163\n",
      "Epoch 50/100 | Train Loss: 0.6710 | Val Loss: 5.3596 | Val RMSE: 2.3151\n",
      "Epoch 51/100 | Train Loss: 0.6688 | Val Loss: 5.3582 | Val RMSE: 2.3148\n",
      "Epoch 52/100 | Train Loss: 0.6682 | Val Loss: 5.3725 | Val RMSE: 2.3179\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "splits = []\n",
    "\"\"\"\n",
    "{\n",
    "    \"k-fold\": {\n",
    "        model,\n",
    "        best_val_loss,\n",
    "        best_val_rmse\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "cv_results = {}\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    X_train = X[train_idx]\n",
    "    X_test = X[test_idx]\n",
    "    y_train = y[train_idx]\n",
    "    y_test = y[test_idx]\n",
    "\n",
    "    # Prepare scaled containers\n",
    "    X_train_scaled = np.zeros_like(X_train)\n",
    "    y_train_scaled = np.zeros_like(y_train)\n",
    "    X_test_scaled = np.zeros_like(X_test)\n",
    "    y_test_scaled = np.zeros_like(y_test)\n",
    "\n",
    "    scaler_X_per_product = []\n",
    "    scaler_y_per_product = []\n",
    "\n",
    "    T, N, F = X_train.shape\n",
    "    for node_idx in range(N):\n",
    "        # Fit a scaler for features of this product\n",
    "        scaler_X = StandardScaler()\n",
    "        scaler_y = StandardScaler()\n",
    "\n",
    "        X_node = X_train[:, node_idx, :]  # (T, F)\n",
    "        y_node = y_train[:, node_idx].reshape(-1, 1)\n",
    "\n",
    "        X_train_scaled[:, node_idx, :] = scaler_X.fit_transform(X_node)\n",
    "        y_train_scaled[:, node_idx] = scaler_y.fit_transform(y_node).ravel()\n",
    "\n",
    "        # Scale test data using same scaler\n",
    "        X_test_scaled[:, node_idx, :] = scaler_X.transform(X_test[:, node_idx, :])\n",
    "        y_test_scaled[:, node_idx] = scaler_y.transform(\n",
    "            y_test[:, node_idx].reshape(-1, 1)\n",
    "        ).ravel()\n",
    "\n",
    "        scaler_X_per_product.append(scaler_X)\n",
    "        scaler_y_per_product.append(scaler_y)\n",
    "\n",
    "    window_size = int(\n",
    "        0.1 * T\n",
    "    )  # TODO: Consider here the implications of window size and the possibility of training a rolling LSTM with no fixed window size\n",
    "    batch_size = 64\n",
    "\n",
    "    train_dataset = GraphTimeSeriesDataset(\n",
    "        X_train_scaled, y_train_scaled, window_size=window_size\n",
    "    )\n",
    "    test_dataset = GraphTimeSeriesDataset(\n",
    "        X_test_scaled, y_test_scaled, window_size=window_size\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    input_size = F  # number of features per node\n",
    "    hidden_size = 16\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = LSTMBaseline(input_size, hidden_size).to(device)\n",
    "\n",
    "    model, history = train_lstm(\n",
    "        model,\n",
    "        train_dataset,\n",
    "        val_dataset=test_dataset,\n",
    "        window_size=5,\n",
    "        batch_size=batch_size,\n",
    "        num_epochs=100,\n",
    "        lr=0.001,\n",
    "        eval_every=1,\n",
    "        patience=20,\n",
    "    )\n",
    "    cv_results[f\"fold_{i}\"] = {\n",
    "        \"model\": model,\n",
    "        \"best_val_loss\": history[\"best_val_loss\"],\n",
    "        \"best_val_rmse\": history[\"best_val_rmse\"],\n",
    "    }\n",
    "\n",
    "# # Print the size of each fold\n",
    "# for i, (X_train_fold, X_test_fold, y_train_fold, y_test_fold) in enumerate(splits):\n",
    "#     print(f\"Fold {i+1}:\")\n",
    "#     print(f\"Training set shape: {X_train_fold.shape}\")\n",
    "#     print(f\"Test set shape: {X_test_fold.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367501aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean best validation loss: 1.2868 ± 0.7231\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert cv_results to a DataFrame\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "\n",
    "# Calculate mean and std of best_val_loss\n",
    "mean_loss = results_df.loc[\"best_val_rmse\"].mean()\n",
    "std_loss = results_df.loc[\"best_val_rmse\"].std()\n",
    "\n",
    "print(f\"Mean best validation loss: {mean_loss:.4f} ± {std_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13176b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = int(0.8 * X.shape[0])\n",
    "\n",
    "# X_train, X_test = X[:train], X[train:]\n",
    "# y_train, y_test = y[:train], y[train:]\n",
    "\n",
    "\n",
    "# half_X, half_y = X[:int(0.5 * X.shape[0])], y[:int(0.5 * X.shape[0])]\n",
    "# train = int(0.8 * half_X.shape[0])\n",
    "# X_train, X_test = half_X[:train], half_X[train:]\n",
    "# y_train, y_test = half_y[:train], half_y[train:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ed04d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_scaled range: -0.43 to 9.22\n",
      "y_scaled range: -0.35 to 9.32\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Flatten for scaling: combine time and nodes\n",
    "T, N, F = X_train.shape\n",
    "X_flat = X_train.reshape(-1, F)  # shape [T*N, F]\n",
    "y_flat = y_train.reshape(-1, 1)  # shape [T*N, 1]\n",
    "\n",
    "# Create scalers\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Fit and transform\n",
    "X_train_scaled_flat = scaler_X.fit_transform(X_flat)\n",
    "y_train_scaled_flat = scaler_y.fit_transform(y_flat)\n",
    "\n",
    "# Reshape back to original shape\n",
    "X_train_scaled = X_train_scaled_flat.reshape(T, N, F)\n",
    "y_train_scaled = y_train_scaled_flat.reshape(T, N)\n",
    "\n",
    "print(f\"X_scaled range: {X_train_scaled.min():.2f} to {X_train_scaled.max():.2f}\")\n",
    "print(f\"y_scaled range: {y_train_scaled.min():.2f} to {y_train_scaled.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f51eb0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_flat = y_test.reshape(-1, 1)\n",
    "y_test_scaled_flat = scaler_y.transform(y_test_flat)\n",
    "y_test_scaled = y_test_scaled_flat.reshape(y_test.shape)\n",
    "\n",
    "X_test_flat = X_test.reshape(-1, F)\n",
    "X_test_scaled_flat = scaler_X.transform(X_test_flat)\n",
    "X_test_scaled = X_test_scaled_flat.reshape(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfe32fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "T, N, F = X_train.shape\n",
    "\n",
    "# Prepare scaled containers\n",
    "X_train_scaled = np.zeros_like(X_train)\n",
    "y_train_scaled = np.zeros_like(y_train)\n",
    "X_test_scaled = np.zeros_like(X_test)\n",
    "y_test_scaled = np.zeros_like(y_test)\n",
    "\n",
    "scaler_X_per_product = []\n",
    "scaler_y_per_product = []\n",
    "\n",
    "for node_idx in range(N):\n",
    "    # Fit a scaler for features of this product\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "\n",
    "    X_node = X_train[:, node_idx, :]  # (T, F)\n",
    "    y_node = y_train[:, node_idx].reshape(-1, 1)\n",
    "\n",
    "    X_train_scaled[:, node_idx, :] = scaler_X.fit_transform(X_node)\n",
    "    y_train_scaled[:, node_idx] = scaler_y.fit_transform(y_node).ravel()\n",
    "\n",
    "    # Scale test data using same scaler\n",
    "    X_test_scaled[:, node_idx, :] = scaler_X.transform(X_test[:, node_idx, :])\n",
    "    y_test_scaled[:, node_idx] = scaler_y.transform(\n",
    "        y_test[:, node_idx].reshape(-1, 1)\n",
    "    ).ravel()\n",
    "\n",
    "    scaler_X_per_product.append(scaler_X)\n",
    "    scaler_y_per_product.append(scaler_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0700be18",
   "metadata": {},
   "source": [
    "# LSTM baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85193d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "window_size = int(\n",
    "    0.1 * T\n",
    ")  # TODO: Consider here the implications of window size and the possibility of training a rolling LSTM with no fixed window size\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = GraphTimeSeriesDataset(\n",
    "    X_train_scaled, y_train_scaled, window_size=window_size\n",
    ")\n",
    "test_dataset = GraphTimeSeriesDataset(\n",
    "    X_test_scaled, y_test_scaled, window_size=window_size\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "103caf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = F  # number of features per node\n",
    "hidden_size = 16\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = LSTMBaseline(input_size, hidden_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4851a6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 1.0161 | Val Loss: 0.6438 | Val RMSE: 0.8024\n",
      " --> Best model saved at epoch 1\n",
      "Epoch 2/100 | Train Loss: 1.0064 | Val Loss: 0.6393 | Val RMSE: 0.7996\n",
      " --> Best model saved at epoch 2\n",
      "Epoch 3/100 | Train Loss: 1.0039 | Val Loss: 0.6329 | Val RMSE: 0.7956\n",
      " --> Best model saved at epoch 3\n",
      "Epoch 4/100 | Train Loss: 1.0008 | Val Loss: 0.6303 | Val RMSE: 0.7939\n",
      " --> Best model saved at epoch 4\n",
      "Epoch 5/100 | Train Loss: 0.9967 | Val Loss: 0.6146 | Val RMSE: 0.7840\n",
      " --> Best model saved at epoch 5\n",
      "Epoch 6/100 | Train Loss: 0.9912 | Val Loss: 0.5998 | Val RMSE: 0.7744\n",
      " --> Best model saved at epoch 6\n",
      "Epoch 7/100 | Train Loss: 0.9797 | Val Loss: 0.5958 | Val RMSE: 0.7719\n",
      " --> Best model saved at epoch 7\n",
      "Epoch 8/100 | Train Loss: 0.9566 | Val Loss: 0.5807 | Val RMSE: 0.7620\n",
      " --> Best model saved at epoch 8\n",
      "Epoch 9/100 | Train Loss: 0.9328 | Val Loss: 0.5394 | Val RMSE: 0.7344\n",
      " --> Best model saved at epoch 9\n",
      "Epoch 10/100 | Train Loss: 0.9176 | Val Loss: 0.5294 | Val RMSE: 0.7276\n",
      " --> Best model saved at epoch 10\n",
      "Epoch 11/100 | Train Loss: 0.9049 | Val Loss: 0.5506 | Val RMSE: 0.7420\n",
      "Epoch 12/100 | Train Loss: 0.8943 | Val Loss: 0.5261 | Val RMSE: 0.7253\n",
      " --> Best model saved at epoch 12\n",
      "Epoch 13/100 | Train Loss: 0.8891 | Val Loss: 0.5281 | Val RMSE: 0.7267\n",
      "Epoch 14/100 | Train Loss: 0.8825 | Val Loss: 0.5116 | Val RMSE: 0.7153\n",
      " --> Best model saved at epoch 14\n",
      "Epoch 15/100 | Train Loss: 0.8791 | Val Loss: 0.5073 | Val RMSE: 0.7123\n",
      " --> Best model saved at epoch 15\n",
      "Epoch 16/100 | Train Loss: 0.8745 | Val Loss: 0.4996 | Val RMSE: 0.7069\n",
      " --> Best model saved at epoch 16\n",
      "Epoch 17/100 | Train Loss: 0.8732 | Val Loss: 0.4981 | Val RMSE: 0.7057\n",
      " --> Best model saved at epoch 17\n",
      "Epoch 18/100 | Train Loss: 0.8656 | Val Loss: 0.5193 | Val RMSE: 0.7206\n",
      "Epoch 19/100 | Train Loss: 0.8602 | Val Loss: 0.4953 | Val RMSE: 0.7038\n",
      " --> Best model saved at epoch 19\n",
      "Epoch 20/100 | Train Loss: 0.8616 | Val Loss: 0.5110 | Val RMSE: 0.7149\n",
      "Epoch 21/100 | Train Loss: 0.8582 | Val Loss: 0.5009 | Val RMSE: 0.7077\n",
      "Epoch 22/100 | Train Loss: 0.8556 | Val Loss: 0.5013 | Val RMSE: 0.7080\n",
      "Epoch 23/100 | Train Loss: 0.8554 | Val Loss: 0.4979 | Val RMSE: 0.7056\n",
      "Epoch 24/100 | Train Loss: 0.8459 | Val Loss: 0.5169 | Val RMSE: 0.7189\n",
      "Epoch 25/100 | Train Loss: 0.8456 | Val Loss: 0.4971 | Val RMSE: 0.7050\n",
      "Epoch 26/100 | Train Loss: 0.8431 | Val Loss: 0.5013 | Val RMSE: 0.7080\n",
      "Epoch 27/100 | Train Loss: 0.8382 | Val Loss: 0.5133 | Val RMSE: 0.7164\n",
      "Epoch 28/100 | Train Loss: 0.8372 | Val Loss: 0.5100 | Val RMSE: 0.7141\n",
      "Epoch 29/100 | Train Loss: 0.8325 | Val Loss: 0.5140 | Val RMSE: 0.7169\n",
      "Epoch 30/100 | Train Loss: 0.8317 | Val Loss: 0.5211 | Val RMSE: 0.7219\n",
      "Epoch 31/100 | Train Loss: 0.8312 | Val Loss: 0.5036 | Val RMSE: 0.7096\n",
      "Epoch 32/100 | Train Loss: 0.8267 | Val Loss: 0.5059 | Val RMSE: 0.7113\n",
      "Epoch 33/100 | Train Loss: 0.8268 | Val Loss: 0.5107 | Val RMSE: 0.7146\n",
      "Epoch 34/100 | Train Loss: 0.8231 | Val Loss: 0.5101 | Val RMSE: 0.7142\n",
      "Epoch 35/100 | Train Loss: 0.8180 | Val Loss: 0.5153 | Val RMSE: 0.7178\n",
      "Epoch 36/100 | Train Loss: 0.8158 | Val Loss: 0.5114 | Val RMSE: 0.7151\n",
      "Epoch 37/100 | Train Loss: 0.8121 | Val Loss: 0.5221 | Val RMSE: 0.7225\n",
      "Epoch 38/100 | Train Loss: 0.8094 | Val Loss: 0.5239 | Val RMSE: 0.7238\n",
      "Epoch 39/100 | Train Loss: 0.8066 | Val Loss: 0.5182 | Val RMSE: 0.7198\n",
      "Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "model, history = train_lstm(\n",
    "    model,\n",
    "    train_dataset,\n",
    "    val_dataset=test_dataset,\n",
    "    window_size=5,\n",
    "    batch_size=batch_size,\n",
    "    num_epochs=100,\n",
    "    lr=0.001,\n",
    "    eval_every=1,\n",
    "    patience=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b00246",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = []\n",
    "all_trues = []\n",
    "all_node_ids = []\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x = batch[\"x\"].to(device)  # [B, window_size, F]\n",
    "        y_true = batch[\"y\"].to(device)  # [B]\n",
    "        node_ids = batch[\"node_id\"]  # [B], keep on CPU\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        all_preds.append(y_pred.cpu())\n",
    "        all_trues.append(y_true.cpu())\n",
    "        all_node_ids.append(node_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50a0434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example predictions on test set:\n",
      "Node 4 | True: -1.21 | Pred: -0.67\n",
      "Node 1 | True: 0.29 | Pred: 0.25\n",
      "Node 40 | True: -0.08 | Pred: 0.01\n",
      "Node 2 | True: 0.45 | Pred: -0.67\n",
      "Node 31 | True: 2.18 | Pred: 0.50\n",
      "Node 28 | True: -0.43 | Pred: -0.54\n",
      "Node 7 | True: -0.68 | Pred: 0.14\n",
      "Node 18 | True: -0.84 | Pred: -0.17\n",
      "Node 27 | True: 1.14 | Pred: 0.45\n",
      "Node 27 | True: -0.77 | Pred: -0.60\n"
     ]
    }
   ],
   "source": [
    "num_examples = 10\n",
    "print(\"Example predictions on test set:\")\n",
    "for i in range(num_examples):\n",
    "    print(\n",
    "        f\"Node {all_node_ids[i].item()} | True: {all_trues[i].item():.2f} | Pred: {all_preds[i].item():.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9eab6",
   "metadata": {},
   "source": [
    "# GCNLSTM baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2671768",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = pd.read_csv(\n",
    "    \"./supplygraph-supply-chain-planning-using-gnns/versions/2/Raw Dataset/Edges/Edges (Plant).csv\"\n",
    ")\n",
    "\n",
    "edges[\"node1\"] = edges[\"node1\"].astype(str)\n",
    "edges[\"node2\"] = edges[\"node2\"].astype(str)\n",
    "\n",
    "edges[\"node1\"] = edges[\"node1\"].map(product_to_id)\n",
    "edges[\"node2\"] = edges[\"node2\"].map(product_to_id)\n",
    "\n",
    "edge_index = edges[[\"node1\", \"node2\"]].to_numpy().T.astype(np.int64)\n",
    "\n",
    "num_nodes = len(product_to_id)\n",
    "\n",
    "train_dataset = StaticGraphTemporalSignal(\n",
    "    edge_index=edge_index,\n",
    "    edge_weight=np.ones(edge_index.shape[1]),\n",
    "    features=X_train_scaled,\n",
    "    targets=y_train_scaled,\n",
    ")\n",
    "\n",
    "test_dataset = StaticGraphTemporalSignal(\n",
    "    edge_index=edge_index,\n",
    "    edge_weight=np.ones(edge_index.shape[1]),\n",
    "    features=X_test_scaled,\n",
    "    targets=y_test_scaled,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "cd9be4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch_geometric_temporal.signal import temporal_signal_split\n",
    "\n",
    "# train_dataset, test_dataset = temporal_signal_split(dataset, train_ratio=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0397eefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric_temporal.nn.recurrent import GCLSTM\n",
    "\n",
    "\n",
    "class RecurrentGCN(torch.nn.Module):\n",
    "    def __init__(self, node_features):\n",
    "        super(RecurrentGCN, self).__init__()\n",
    "        self.recurrent = GCLSTM(node_features, 32, 1)\n",
    "        self.linear = torch.nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight):\n",
    "        h, c = self.recurrent(x, edge_index, edge_weight)  # Unpack (H, C)\n",
    "        h = F.relu(h)\n",
    "        h = self.linear(h)\n",
    "        return h.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c651b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RecurrentGCN(node_features=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4047ebc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [00:10<01:43,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100\n",
      "Train MSE: 0.9739\n",
      "Test MSE: 2.4134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 20/100 [00:21<01:33,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20/100\n",
      "Train MSE: 0.9727\n",
      "Test MSE: 2.4125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 30/100 [00:32<01:20,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30/100\n",
      "Train MSE: 0.9714\n",
      "Test MSE: 2.4114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 40/100 [00:41<01:08,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40/100\n",
      "Train MSE: 0.9708\n",
      "Test MSE: 2.4112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 50/100 [00:54<01:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50/100\n",
      "Train MSE: 0.9690\n",
      "Test MSE: 2.4119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 60/100 [01:05<00:49,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60/100\n",
      "Train MSE: 0.9677\n",
      "Test MSE: 2.4120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 70/100 [01:15<00:36,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70/100\n",
      "Train MSE: 0.9668\n",
      "Test MSE: 2.4114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 80/100 [01:25<00:23,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 80/100\n",
      "Train MSE: 0.9659\n",
      "Test MSE: 2.4127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 90/100 [01:36<00:12,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 90/100\n",
      "Train MSE: 0.9652\n",
      "Test MSE: 2.4130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [01:48<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 100/100\n",
      "Train MSE: 0.9650\n",
      "Test MSE: 2.4128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def calculate_mse(model, dataset):\n",
    "    \"\"\"Calculate MSE on a dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for time, snapshot in enumerate(dataset):\n",
    "            y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "            total_loss += torch.mean((y_hat - snapshot.y) ** 2).item()\n",
    "    model.train()\n",
    "    return total_loss / (time + 1)\n",
    "\n",
    "\n",
    "n_epoch = 100\n",
    "for epoch in tqdm(range(n_epoch)):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for time, snapshot in enumerate(train_dataset):\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model(snapshot.x, snapshot.edge_index, snapshot.edge_attr)\n",
    "        loss = torch.mean((y_hat - snapshot.y) ** 2)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        train_mse = calculate_mse(model, train_dataset)\n",
    "        test_mse = calculate_mse(model, test_dataset)\n",
    "        print(f\"\\nEpoch {epoch + 1}/{n_epoch}\")\n",
    "        print(f\"Train MSE: {train_mse:.4f}\")\n",
    "        print(f\"Test MSE: {test_mse:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gml_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
